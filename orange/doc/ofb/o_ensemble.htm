<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css" MEDIA=screen>
</HEAD>
<body>

<p class="Path">
Prev: <a href="o_fss.htm">Feature subset selection</a>,
Next: <a href="domain.htm">Basic data manipulation</a>,
Up: <a href="other.htm">Other Techniques for Orange Scripting</a>
</p>

<H1>Ensemble Techniques</H1>

<p>Building ensemble classifiers in Orange is simple and
easy. Statring from learners/classifiers that can predict
probabilities and, if needed, use example weights, ensembles are
acctually wrappers that can aggregate decisions of other
classifiers. These wrappers in essence learners/classifiers and can be
used just as any similar Orange objects. In this lesson, we will first
see how easy it is ot use a module for bagging and boosting that is
included in Orange distribution (<a
href="../modules/orngEnsemble.htm">orngEnsemble</a> module), and then
for a somehow more interesting example build our own ensemble
learner.</p>

<h2>Ensemble learning using orngEnsemble</h2>

<p>First, there is a module for <a
href="../modules/orngEnsemble.htm">Bagging and Boosting</a>, and using
it is very easy: you have to define a learner, give it to bagger or
booster, which in turn returns a new (boosted or bagged) learner. Here
goes an example:</p>

<p class="header"><a href="ensemble3.py">ensemble3.py</a> (uses <a href=
"promoters.tab">promoters.tab</a>)</p>
<xmp class=code>import orange, orngTest, orngStat, orngEnsemble
data = orange.ExampleTable("promoters")

majority = orange.MajorityLearner()
majority.name = "default"
knn = orange.kNNLearner(k=11)
knn.name = "k-NN (k=11)"

bagged_knn = orngEnsemble.BaggedLearner(knn, t=10)
bagged_knn.name = "bagged k-NN"
boosted_knn = orngEnsemble.BoostedLearner(knn, t=10)
bagged_knn.name = "boosted k-NN"

learners = [majority, knn, bagged_knn, boosted_knn]
results = orngTest.crossValidation(learners, data, folds=10)
print "        Learner   CA     Brier Score"
for i in range(len(learners)):
    print ("%15s:  %5.3f  %5.3f") % (learners[i].name,
        orngStat.CA(results)[i], orngStat.BrierScore(results)[i])
</xmp>

<p>Most of the code is used for defining and naming objects that learn, and the last piece of code is to report evaluation results. Notice that to bag or boost a learner, it takes only a single line of code (like, <code>bagged_knn = orngEnsemble.BaggedLearner(knn, t=10)</code>)! Parameter <code>t</code> in bagging and boosting refers to number of classifiers that will be used for voiting (or, if you like better, number of iterations by boosting/bagging). Depending on your random generator, you may get something like:</p>

<XMP class=code>        Learner   CA     Brier Score
        default:  0.473  0.501
    k-NN (k=11):  0.859  0.242
   boosted k-NN:  0.813  0.267
boosted classifier:  0.858  0.249

        Learner   CA     Brier Score
        default:  0.472  0.501
    k-NN (k=11):  0.821  0.340
   boosted k-NN:  0.868  0.231
       boosting:  0.849  0.373
</XMP>


<h2>Build You Own Ensemble Learner</h2>

<p>If you have sequentially followed through this tutorial,
building your own ensemble learner is nothing new: you have
already build a <a href="c_bagging.htm">module for bagging</a>.
Here is another similar example: we will build a learner that
takes a list of learners, obtains classifiers by training them
on the example set, and when classifying, uses classifiers to
estimate probabilities and goes with the class that is predicted
the highest probability. That is, at the end, the prediction of
a sole classifier counts. If class probabilities are requested,
they are reported as computed by this very classifier.</p>

<p>Here is the code that implements our learner and classifier:</p>

<p class="header">part of <a href="ensemble2.py">ensemble2.py</a></p>
<xmp class=code>def WinnerLearner(examples=None, **kwds):
  learner = apply(WinnerLearner_Class, (), kwds)
  if examples:
    return learner(examples)
  else:
    return learner

class WinnerLearner_Class:
  def __init__(self, name='winner classifier', learners=None):
    self.name = name
    self.learners = learners

  def __call__(self, data, learners=None, weight=None):
    if learners:
      self.learners = learners
    classifiers = []
    for l in self.learners:
      classifiers.append(l(data))
    return WinnerClassifier(classifiers = classifiers)

class WinnerClassifier:
  def __init__(self, **kwds):
    self.__dict__ = kwds

  def __call__(self, example, resultType = orange.GetValue):
    pmatrix = []
    for c in self.classifiers:
      pmatrix.append(c(example, orange.GetProbabilities))

    maxp = []  # stores max class probabilities for each classifiers
    for pv in pmatrix:
      maxp.append(max(pv))
      
    p = max(maxp)  # max class probability
    classifier_index = maxp.index(p)
    c = pmatrix[classifier_index].modus()
    
    if resultType == orange.GetValue:
      return c
    elif resultType == orange.getClassDistribution:
      return pmatrix[classifier_index]
    else:
      return (c, pmatrix[classifier_index])
</xmp>

<p><code>WinnerLearner_Class</code> store the learners and, when called with a data set, constructs the classifiers and passes them to <code>WinnerClassifier</code>. When this is called with a data instance, it computes class probabilities, finds a maximum probability for some class, and accordingly reports on either the class, probabilities or both. Notice that we have also took care that this learner/classifier confirms to everything that is expected from such objects in Orange, so it may be used by other modules, most importantly for classifier validation.</p>

<p>An example of how our new learner is used is exemplified by the following script:

<p class="header">part of <a href="ensemble2.py">ensemble2.py</a> (uses <a href=
"promoters.tab">promoters.tab</a>)</p>
<xmp class=code>tree = orngTree.TreeLearner(mForPruning=5.0)
tree.name = 'class. tree'
bayes = orange.BayesLearner()
bayes.name = 'naive bayes'
winner = WinnerLearner(learners=[tree, bayes])
winner.name = 'winner'

majority = orange.MajorityLearner()
majority.name = 'default'
learners = [majority, tree, bayes, winner]

data = orange.ExampleTable("promoters")

results = orngTest.crossValidation(learners, data)
print "Classification Accuracy:"
for i in range(len(learners)):
    print ("%15s: %5.3f") % (learners[i].name, orngStat.CA(results)[i])
</xmp>

<p>Notice again that invoking our new objects and using them for machine learning is just as easy as using any other learner. When running this script, it may report something like:</p>

<XMP class=code>Classification Accuracy:
        default: 0.472
    class. tree: 0.830
    naive bayes: 0.868
         winner: 0.877
</XMP>

<p>The example script above that implements WinnerLearner and WinnerClassifiers may easily be adapted to something that you need for ensemble learner. For an exercise, change the learner such that it uses cross-validation to estimate the probability that the classifier will predict correctly, and when classifying use this probability to weight the classifiers and change winner-schema to voting.
</P>

<hr><br><p class="Path">
Prev: <a href="o_fss.htm">Feature subset selection</a>,
Next: <a href="domain.htm">Basic data manipulation</a>,
Up: <a href="other.htm">Other Techniques for Orange Scripting</a>
</p>


</body></html>

