<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
</HEAD>
<body>

<h1>orngStat: Orange Statistics for Classifiers</h1>


<P>This module contains various measures of quality, such as classification accuracy, ROC statistics and similar. Most functions require an argument named <code>res</code>, an instance of <code>ExperimentResults</code> as computed by functions from <a href="orngTest.htm">orngTest</a> and which contains classifications and probabilities obtained through cross-validation, leave one out, testing on training or new examples...</P>

<P>If examples are weighted, weights are taken into account. This can be disabled by giving <code>unweighted=1</code> as a keyword argument. Another way of disabling weights is to clear the <code>ExperimentResults</code>' flag <code>weights</code>.</P>

<H2>General Measures of Quality</H2>

<DL>

<DT><B>CA(res, reportSE=0)</B></DT>

<DD>Computes classification accuracy, i.e. percentage of matches
between predicted and actual class. The function returns a list of
classification accuracies, or a list of tuples with classification
accuracies and standard errors.

If results are from multiple repetitions of experiments (like those
returned by orngTest.crossValidation or orngTest.proportionTest) the
standard error (SE) is estimated from deviation of classification
accuracy accross folds (SD), as SE = SD/sqrt(N), where N is number of
repetitions (e.g. number of folds).

If results are from a single repetition, we assume independency of
examples and treat the classification accuracy as distributed
according to binomial distribution. This can be approximated by normal
distribution, so we report the SE of sqrt(CA*(1-CA)/N), where CA is
classification accuracy and N is number of test examples.  </DD><P>

<DT><B>AP(res)</B></DT>
<DD>Computes the average probability assigned to the correct class.</DD><P>

<DT><B>BrierScore(res)</B></DT>
<DD>Computes the Brier's score, defined as the average (over test
examples) of sum<SUB>x</SUB>(t(x)-p(x))<SUP>2</SUP>, where x is a
class, t(x) is 1 for the correct class and 0 for the others, and p(x)
is the probability that the classifier assigned to the class
x.</DD><P>

<DT><B>IS(res, apriori=None)</B></DT>
<DD>Computes information scores as defined by Kononenko and Bratko
(1991). Argument 'apriori' gives the apriori class distribution; if it
is omitted, the class distribution is computed from the actual classes
of examples in res.</DD><P>

<DT><B>CA_se(res)</B></DT>
<DD>Deprecated. Same as CA(res, reportSE=1).</DD>

</DL>


<H2>Analysis of Confusion Matrix</H2>

<DL>

<DT><B>computeConfusionMatrices(res, classIndex=1, {cutoff})</B></DT>
<DD>A utility function that computes a 2-class confusion matrix from
ExperimentResults. The target class is the class attribute's
<CODE>baseValue</CODE>, but it can be overridden by argument
<CODE>classIndex</CODE>. If there is neither <CODE>baseValue</CODE>
nor <CODE>classIndex</CODE>, 1 is used as default. If keyword argument
cutoff is given (e.g. computeConfusionMatrices(results, cutoff=0.3),
the predicted probabilities are used instead of class prediction and
the given value is used as a threshold: if predicted probability of
the target class is higher than cutoff, the target class is
predicted.<P>

The function returns a list of instances of ConfusionMatrix is returned, containing the (weighted) number of true positives (<code>TP</code>), false negatives (<code>FN</code>), false positives (<code>FP</code>) and true negatives (<code>TN</code>).


</DD><P>


<DT><B>sens(confm), spec(confm), PPV(confm), NPV(confm)</B></DT>
<DD>Compute sensitivity [TP/(TP+FN)], specificity [TN/(TN+FP)],
positive predictive value [TP/(TP+FP)] and negative predictive value
[TN/(TN+FN)] from confusion matrix. If confm is a single confusion
matrix, a single result is returned. If confm is a list of confusion
matrices, a list of sensitivities is returned.

<P>Note that weights are taken into account when computing the matrix, so
these functions don't check the 'weighted' keyword argument.</p></DD><P>
</DL>


<H2>ROC Analysis</H2>

<P>ROC analysis was initially developed for a binary-like problems and there is no consensus on how to apply it in multi-class problems, nor do we know for sure how to do ROC analysis after cross validation and similar multiple sampling techniques. If you are interested in the area under the curve, function <code>AUC</code> will deal with those problems as specifically described below.</P>

<P>Other functions, which plot the curves and statistically compare them, require that the results come from a test with a single iteration, and they always compare one chosen class against all others. If you have cross validation results, you can either use <a href="#splitbyiterations"><code>splitByIterations</code></a> to split the results by folds, call the function for each fold separately and then sum the results up however you see fit, or you can set the <code>ExperimentResults</code>' attribute <code>numberOfIterations</code> to 1, to cheat the function - at your own responsibility for the statistical correctness. Regarding the multi-class problems, if you don't chose a specific class, <code>orngStat</code> will use the class attribute's <CODE>baseValue</CODE> at the time when results were computed. If <code>baseValue</code> was not given at that time, 1 (that is, the second class) is used as
default.</P>

<DL>
<DT><B>AUC(res, method = AUC.ByWeightedPairs)</B></DT>
<DD>Returns the area under ROC curve (AUC) given a set of experimental results. For multivalued class problems, it will compute some sort of average, as specified by the argument <code>method</code>:
  <dl>
  <dt><code>AUC.ByWeightedPairs</code> (or <code>0</code>)</dt>
  <dd>Computes AUC for each pair of classes and averages the results, weighting them by the number of pairs of examples from these two classes (e.g. by the product of probabilities of the two classes). AUC computed in this way still behaves as concordance index, e.g., gives the probability that two randomly chosen examples from different classes will be correctly recognized (this is of course true only if the classifier <em>knows</em> from which two classes the examples came).</dd>

  <dt><code>AUC.ByPairs</code> (or <code>1</code>)</dt>
  <dd>Similar as above, except that the average over class pairs is not weighted. This AUC is, like the binary, independent of class distributions, but it is not related to concordance index any more.</dd>

  <dt><code>AUC.WeightedOneAgainstAll</code> (or <code>2</code>)</dt>
  <dd>For each class, it computes AUC for this class against all others (that is, treating other classes as one class). The AUCs are then averaged by the class probabilities. This is related to concordance index in which we test the classifier's (average) capability for distinguishing the examples from a specified class from those that come from other classes. Unlike the binary AUC, the measure is not independent of class distributions.</dd>

  <dt><code>AUC.OneAgainstAll</code> (or <code>3</code>)</dt>
  <dd>As above, except that the average is not weighted.</dd>
  </dl>

<P>In case of <em>multiple folds</em>, for instance, it the data comes from cross validation, the computation goes like this. When computing the partial AUCs for individual pairs of classes or singled-out classes, AUC is computed for each fold separately and then averaged (ignoring the number of examples in each fold, it's just a simple average). However, if a certain fold doesn't contain any examples of a certain class (from the pair), the partial AUC is computed treating the results as coming from a single-fold. This is not really correct since the class probabilities from different folds are not necessarily comparable, yet this will most often occur in a leave-one-out experiments, comparability shouldn't be a problem.</P>
</dd>


<DT><B>AUCWilcoxon(res, classIndex=1)</B></DT>
<DD>Computes the area under ROC (AUC) and its standard error using
Wilcoxon's approach proposed by Hanley and McNeal (1982). This
function expects experimental data from a single iteration. The result
is a list of tuples (aROC, standard error).</DD><P>

<DT><B>compare2AUCs(res, learner1, learner2, classIndex=1)</B></DT>

<DD>Compares ROC curves of learning algorithms with indices learner1
and learner2. ExperimentResults must describe a single-iteration
experiment. The function returns three tuples with areas under ROCs
and standard errors, and the difference of the areas and its standard
error: ((AUC1, SE1), (AUC2, SE2), (AUC1-AUC2,
SE(AUC1)+SE(AUC2)-2*COVAR)).</DD><P>

<DT><B>computeROC(res, classIndex=1)</B></DT>

<DD>Computes a ROC curve as a list of (x, y) tuples, where x is
1-specificity and y is sensitivity. This function expects experimental
data from a single iteration. </DD><P>

<DT><B>computeCDT(res, classIndex=1), ROCsFromCDT(cdt, {print})</B></DT>

<DD><em>These two functions are obsolete and shouldn't be called. Use <code>AUC</code> instead.</em></DD><P>

<DT><B>AROC(res, classIndex=1), AROCFromCDT(res, {print}), compare2AROCs(res, learner1, learner2, classIndex=1)</B></DT>
<DD><em>These are all deprecated, too. Instead, use AUCWilcoxon (for AROC),
AUC (for AROCFromCDT), and compare2AUCs (for compare2AROCs).</em></dd><p>

</DL>

<H2>Comparison of Algorithms</H2>

<DL>
<DT><B>McNemar(res)</B></DT>
<DD>Computes a triangular matrix with McNemar statistics for each pair of classifiers. The statistics is distributed by chi-square distribution with one degree of freedom; critical value for 5% significance is around 3.84.</DD><P>

<DT><B>McNemarOfTwo(res, learner1, learner2)</B></DT>
<DD>McNemarOfTwo computes a McNemar statistics for a pair of classifier, specified by indices learner1 and learner2.</DD><P>

</DL>

<H2>Plotting Functions</H2>

&lt;&lt;to be written&gt;&gt;

<H2>Utility Functions</H2>

<DL>
<DT><B>splitByIterations(res)</B></DT>
<DD>Splits ExperimentResults of multiple iteratation test into a list of ExperimentResults, one for each iteration.
</DD></DL>



<H1>References</H1>

<p>Kononenko I, Bratko I. (1991) Information-based evaluation criterion for
classifier's performance, Machine Learning 1(6): 67-80.</P>

<p>McNeal H (1982). The meaning and Use of the Area under a
Receiver Operating Characteristic (TOC) Curve, Radiology 143: 29-36, April
1982.</p>

<P>Mingers J (1989) An empirical evaluation of selection measures for
decision-tree induction, Machine Learning 3: 319-342.</p>
</BODY>
