<html>
<head>
<link rel=stylesheet href="../style.css" type="text/css" media=screen>
</head>
<body>

<h1>orngTree: Orange Decision Trees Module</h1>

<P>Module orngTree implements a wrapper function
<code>TreeLearner</code> for building both decision and regression
trees, and two functions for tree output. Note that there is a
function called <code>TreeLearner</code> in core Orange which should
not be confused with the one in orngTree module.</p>

<p>For the tree output, the module implements a textual printout and
saving of the tree in the DOT format, that is, in a graph description
language as defined and used within <a
href="http://www.research.att.com/sw/tools/graphviz">GraphViz</a>).</p>

<h2>TreeLearner</h2>

<p><code>TreeLearner</code> is a class that assembles the generic
classification tree learner (from Orange's objects for induction of
decision trees). It sets a number of parameters used in induction that
can also be set after the creation of the object, most often through
the object's attributes. If upon initialization
<code>TreeLearner</code> is given a set of examples, then an instance
of <code>TreeClassifier</code> object is returned instead.

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>examples</dt>
  <dd>Set of training example from which a classification tree is
  induced. If this attribute is provided to a learner, the call
  returns a classifier (default: None).</dd>

  <dt>weightID</dt>
  <dd>Only used if examples are provided. Specifies an ID of a meta
  attribute which holds example weights (default: 0).</dd>

  <dt>binarization</dt>
  <dd>If True the induction constructs binary trees (default: False).</dd>

  <dt>mForPruning</dt>
  <dd>If non-zero, invokes an error-based bottom-up post-pruning,
  where m-estimate is used to estimate class probabilities (default: 0).</dd>

  <dt>sameMajorityPruning</dt>
  <dd>If true, invokes a bottom-up post-pruning by removing the
  subtrees of which all leaves classify to the same class 
  (default: False).</dd>

  <dt>measure</dt>
  <dd>Measure for scoring of the attributes when deciding which of the
  attributes will be used for splitting of the example set in the node.
  Can take one of the following values: "infoGain", "gainRatio", "gini",
  "relief" (default: "gainRatio").</dd>

  <dt>worstAcceptable</dt>
  <dd>Used in pre-pruning, sets the lowest required attribute
  score. If the score of the best attribute is below this margin, the
  tree at that node is not grown further (default: 0).</dd>

  <dt>minSubset</dt>
  <dd>Minimal number of examples in
  non-null leaves (default: 0).</dd>

  <dt>split</dt>
  <dd>Defines a function that will be used in place of Orange's
  <code>TreeSplitConstructor</code> (see <a
  href="../reference/TreeLearner.htm">documentation on
  TreeLearner</a>). Useful when prototyping new tree induction
  algorithms. When this parameter is defined, other parameters that
  affect the procedures for growing of the tree are ignored. These
  include <code>binarization</code>, <code>measure</code>,
  <code>worstAcceptable</code> and <code>minSubset</code> (default:
  None).</dd>

  <dt>minExamples</dt>
  <dd>Data subsets with less than <code>minExamples</code>
  examples are not split any further, that is, all leaves in the tree
  will contain at least that many of examples (default: 0).</dd>

  <dt>maxMajority</dt>
  <dd>Induction stops when the proportion of majority class in the
  node exceeds the value set by this parameter(default: 1.0).</dd>

  <dt>stop</dt>
  <dd>Used for passing a function which is used in place of
  <code><code>TreeStopCriteria</code>. Useful when prototyping new
  tree induction algorithms. See a documentation on <a
  href="../reference/TreeLearner.htm">TreeStopCriteria</a> for more
  info on this function. When used, parameters
  <code>maxMajority</code> and <code>minExamples</code> will not be
  considered (default: None).</dd>

  <dt>storeDistributions, storeContingencies, storeExamples,
  storeNodeClassifier</dt>
  <dd>Determines whether to store class distributions, contingencies and
  examples in TreeNodes, and whether the nodeClassifier should be
  build for internal nodes. By default everything except storeExamples
  is enabled. You won't save any memory by not storing distributions
  but storing contingencies, since distributions actually points to
  the same distribution that is stored in
  <code>contingency.classes.</code>(default: True except for
  storeExamples, which defaults to False).</dd>
</dl>


<h2>printTxt</h2>

Prints out the decision tree in plain text.

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>tree</dt>
  <dd>The tree to be printed out.</dd>

  <dt>fileName</dt>
  <dd>Redirect output to a specified file. If not given or None, a
  standard output is used as printing device (default: None).</dd>

  <dt>examplesLimit</dt>
  <dd>A subtree rooted at the node will only be printed if the number
  of examples which it contained during the induction is above the
  margin set by this parameter (default: 0)</dd>

  <dt>depthLimit</dt>
  <dd>If defined, sets the maximum depth to which the tree will be printed (default: None)</dd>

  <dt>baseValueIndex</dt>
  <dd>Index of the target class value. If None, no related information
  is reported (default: None)</dd>

  <dt>internalNodeFields</dt>
  <dd>A list specifying what to be reported on for non-leaf nodes of
  the tree. Possible elements that can be included in this list are
  <code>'contingency'</code> (probability of majority class),
  <code>'major'</code> (majority class label) and
  <code>'baseValue'</code> for classification trees, and
  <code>'average'</code> and <code>'confidenceInterval'</code> for
  regression problems (default: [])</dd>

  <dt>leafFields</dt>
  <dd>Similar as <code>internalNodeFields</code>, but applies to leaf
  nodes of the tree (default: ["major"] for classification and
  ["average"] for regression trees)</dd>

  <dt>confidenceLevel</dt>
  <dd>Sets the confidence level from which to determine the confidence
  interval when reporting it for regression trees
  (default: 0.95)</dd>

  <dt>decimalPlaces</dt>
  <dd>Number of decimal places used when print out probabilities and
  alike (default: 3)</dd>
</dl>

<h2>printDot</h2>

<p>Prints the tree to a file in a format used by <a
href="http://www.research.att.com/sw/tools/graphviz">GraphViz</a>.
Uses the same parameters as <code>printTxt</code> defined above, and
in addition two parameters which define the shape used for internal
nodes and laves of the tree:

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>leafShape</dt>
  <dd>Shape of the outline around leves of the tree. If "plaintext",
  no outline is used (default: "plaintext")</dd>

  <dt>internalNodeShape</dt>
  <dd>Shape of the outline around internal nodes of the tree. If "plaintext",
  no outline is used (default: "box")</dd>
</dl>

<p>Check <a
href="http://www.research.att.com/~erg/graphviz/info/shapes.html">Polygon-based
Nodes</a> for various outlines supported by GraphViz.</p>

<h2>countNodes</h2>

<p>Returns the number of nodes of tree.</p>

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>tree</dt>
  <dd>The tree for which to count the nodes.</dd>
</dl>

<h2>countLeaves</h2>

<p>Returns the number of leaves in the tree.</p>

<p class=section>Attributes</p>
<dl class=attributes>
  <dt>tree</dt>
  <dd>The tree for which to count the leaves.</dd>
</dl>

<hr>

<H2>Examples</H2>

In the first example we show a simple way of controlling the splitting by allowing to split only when
gainRatio (the default measure) is grater than 0.6.
<p class="header"><a href="tree1.py">tree1.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<XMP class=code>import orange, orngTree
data = orange.ExampleTable("iris.tab")
l = orngTree.TreeLearner(data, worstAcceptable=0.6)
orngTree.printTxt(l, leafFields=['major', 'contingency'])
</XMP>

<p>The result is printed as a plain text as follows:</p>

<XMP class=code>petal width <0.800: Iris-setosa (100.0%)
petal width >=0.800:
|   petal width <1.750:
|   |   petal length <4.950:
|   |   |   sepal length <5.150: Iris-versicolor (80.0%)
|   |   |   sepal length >=5.150: Iris-versicolor (100.0%)
|   |   petal length >=4.950: Iris-virginica (66.66%)
|   petal width >=1.750:
|   |   petal length <4.950: Iris-virginica (83.33%)
|   |   petal length >=4.950: Iris-virginica (100.0%)
</XMP>

<p>The following example shows how to handle stopping criteria by setting the maximal proportion of majority class in the node.</p>

<p class="header"><a href="tree2.py">tree2.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<XMP class=code>import orange, orngTree
data = orange.ExampleTable("iris.tab")

print "BIG TREE:",
tree1 = orngTree.TreeLearner(data)
orngTree.printTxt(tree1, leafFields=['major', 'contingency'])

print "\nPRE-PRUNED TREE:",
tree2 = orngTree.TreeLearner(data, maxMajority=0.7)
orngTree.printTxt(tree2, leafFields=['major', 'contingency'])
</XMP>
<p>
This script prints out two trees, one build using default parameters and the other one pre-pruned with <code>maxMajority</code> set to 70%.
</p>

<XMP class=code>BIG TREE:
petal width <0.800: Iris-setosa (100.0%; <50, 0, 0>)
petal width >=0.800:
|   petal width <1.750:
|   |   petal length <5.350:
|   |   |   petal length <4.950:
|   |   |   |   petal width <1.650: Iris-versicolor (100.0%; <0, 47, 0>)
|   |   |   |   petal width >=1.650: Iris-virginica (100.0%; <0, 0, 1>)
|   |   |   petal length >=4.950:
|   |   |   |   petal width <1.550: Iris-virginica (100.0%; <0, 0, 2>)
|   |   |   |   petal width >=1.550: Iris-versicolor (100.0%; <0, 2, 0>)
|   |   petal length >=5.350: Iris-virginica (100.0%; <0, 0, 2>)
|   petal width >=1.750:
|   |   petal length <4.850:
|   |   |   sepal width <3.100: Iris-virginica (100.0%; <0, 0, 2>)
|   |   |   sepal width >=3.100: Iris-versicolor (100.0%; <0, 1, 0>)
|   |   petal length >=4.850: Iris-virginica (100.0%; <0, 0, 43>)

PRE-PRUNED TREE:
petal width <0.800: Iris-setosa (100.0%; <50, 0, 0>)
petal width >=0.800:
|   petal width <1.750: Iris-versicolor (90.74%; <0, 49, 5>)
|   petal width >=1.750: Iris-virginica (97.82%; <0, 1, 45>)
</XMP>

<p>The next example demonstrates how to write your own stop function. The example itself is more
funny than useful. It prints out two trees. For the first one we define the <code>defStop</code>
function, which is used by default, and combine it with a random function so that the stop criteria
will also be met in additional 20% of the cases when <code>defStop</code> is false. The second tree is build such that it considers only the random function as the stopping criteria. Note that in the second case lambda function still has three parameters, since this is a necessary number of parameters for the stop function (for more, see section on <a href="../reference/TreeLearner.htm">Orange Trees</a> in Orange Reference).
</p>
<p class="header"><a href="tree3.py">tree3.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>

<XMP class=code>import orange, orngTree
from whrandom import randint, random

data = orange.ExampleTable("iris.tab")

defStop = orange.TreeStopCriteria()
f = lambda examples, weightID, contingency: defStop(examples, weightID, contingency) or randint(1, 5)==1
l = orngTree.TreeLearner(data, stop=f)
orngTree.printTxt(l, leafFields=['major', 'contingency'])

f = lambda x,y,z: randint(1, 5)==1
l = orngTree.TreeLearner(data, stop=f)
orngTree.printTxt(l, leafFields=['major', 'contingency'])
</XMP>

<p>The output is not shown here since the resulting trees are rather
big, and they change in time (Python's <code>whrandom</code> library sets the seed
according to the time a script imports this library -- check it out by
running the above script several times).</p>

<p>The next script prints out the data instances at each leaf. It can
do so by setting storeExamples to true when learning the tree: this
causes the <code>TreeLearner</code> to remember the examples at each
node. Notice that we could print out the examples in internal nodes of
the tree (you may do this by a minor change in the script below). The
script also prints out the class distribution at the root node. Again,
class distributions are saved for all the nodes in the tree: this is
enabled by setting <code>storeContingencies</code> to
<code>true</code> (in Python, setting it to 1).</p>

<p class="header"><a href="tree4.py">tree4.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<XMP class=code>import orange, orngTree

def printExamples(node):
    if node:
        if node.branches:
            for b in node.branches:
                printExamples(b)
        else:
            print "----------------- NEW NODE -----------------"
            for ex in node.examples:
                print ex
    else:
        print "null node"


data = orange.ExampleTable("iris.tab")
print len(data)

tree = orngTree.TreeLearner(data, storeExamples=1, storeContingencies=1)
print 'CLASSIFICATION TREE:',
orngTree.printTxt(tree)

print '\nEXAMPLES IN NODES:'
printExamples(tree.tree)

print '\nCONTINGENCY:'
print tree.tree.contingency.classes
</XMP>

<p>The following scripts demonstrate the use of both print functions, i.e., the print functions that prints to the console and the one that prints to the DOT file. Let us first print out a small tree induced from iris data set:</p>

<p class="header"><a href="tree5.py">tree5.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>

<XMP class=code>import orange, orngTree
data = orange.ExampleTable("iris.tab")

tree = orngTree.TreeLearner(data, worstAcceptable=0.6)
orngTree.printTxt(tree)
orngTree.printDot(tree, fileName="tree5.dot")
</XMP>

<P>
This script outputs the tree in the following form:
</P>

<XMP class=code>petal width <0.800: Iris-setosa (100.0%)
petal width >=0.800:
|   petal width <1.750:
|   |   petal length <5.350: Iris-versicolor (94.23%)
|   |   petal length >=5.350: Iris-virginica (100.0%)
|   petal width >=1.750: Iris-virginica (97.82%)
</XMP>

<P>It also writes out a DOT file tree5.dot. This file can be used <a href="http://www.research.att.com/sw/tools/graphviz/">AT&amp;T's Graphviz</a> software to graphically present the tree. Graphwiz's DOT can use different output formats; we have used GIF a obtained the gile tree5.gif with a following command from the console:</P>

<XMP class=code>dot -Tgif tree5.dot -otree5.gif
</XMP>

<P>You can use any of the common viewers for this file, and see something like the following presentation of the tree:</P>

<img src="tree5.gif" alt="tree5.gif" border="0">


<P>Now, we generate the tree without pre-pruning that uses <code>worstAcceptable=0.6</code>, and print it out tree only to the 3rd level. We also print out class distributions (contingencies) and a proportion (percentages) of examples that belong to the majority class in internal nodes.</P>

<p class="header"><a href="tree6.py">tree6.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>

<XMP class=code>import orange, orngTree
data = orange.ExampleTable("iris.tab")

tree = orange.TreeLearner(data)
orngTree.printTxt(tree, internalNodeFields=['contingency','major'], depthLimit=3)
orngTree.printDot(tree, fileName="tree6.dot", internalNodeFields=['contingency','major'], depthLimit=3)
</XMP>

<P>Plain text output of this script is:</P>

<XMP class=code>petal width (33.33%; <50, 50, 50>) <0.800: Iris-setosa (100.0%)
petal width (33.33%; <50, 50, 50>) >=0.800:
|   petal width (50.0%; <0, 50, 50>) <1.750:
|   |   petal length (90.74%; <0, 49, 5>) <4.950:
|   |   petal length (90.74%; <0, 49, 5>) >=4.950:
|   petal width (50.0%; <0, 50, 50>) >=1.750:
|   |   petal length (97.82%; <0, 1, 45>) <4.950:
|   |   petal length (97.82%; <0, 1, 45>) >=4.950:
</XMP>

<P>The corresponding GIF file we have created using dot is:</P>

<img src="tree6.gif" alt="tree6.gif" border="0">

<P>And finally, and also to indicate that orngTree can also handle regression trees, let us build a regression tree and print it out. To do this, we use housing data set that uses continuous class.</P>

<p class="header"><a href="tree7.py">tree7.py</a> (uses <a href=
"housing.tab">housing.tab</a>)</p>

<XMP class=code>import orange, orngTree

train = orange.ExampleTable("housing.tab")

l = orngTree.TreeLearner(train, measure="retis", mForPruning=2, minExamples=20)

orngTree.printTxt(l, leafFields=["average","confidenceInterval"], decimalPlaces=1, confidenceLevel=0.85)
orngTree.printDot(l, leafFields=["average","confidenceInterval"], fileName="tree7.dot", decimalPlaces=1, confidenceLevel=0.85)
print
print "Number of nodes:",orngTree.countNodes(l)
print "Number of leaves:",orngTree.countLeaves(l)
</XMP>
<p>
The result is the following tree in text format:
</p>

<XMP class=code>RM <6.941: 19.9 [19.5, 20.4]
RM >=6.941:
|   RM <7.437:
|   |   CRIM <7.393:
|   |   |   DIS <1.886: 45.7 [39.4, 51.9]
|   |   |   DIS >=1.886: 32.7 [31.9, 33.6]
|   |   CRIM >=7.393: 14.4 [11.3, 17.5]
|   RM >=7.437:
|   |   TAX <534.500: 45.9 [44.7, 47.1]
|   |   TAX >=534.500: 21.9 [21.9, 21.9]

Number of nodes: 11
Number of leaves: 6
</XMP>

<p>Notice that instead of class labels, each node has a corresponding
average value of class variable (price of housing) over the instances
from training set that belong to this node. A corresponding tree in
GIF format (again, using Graphwiz's dot) is:</p>

<img src="tree7.gif" alt="tree7.gif" border="0">

<HR>

<H2>References</H2>

<P>E Koutsofios, SC North. Drawing Graphs with dot. AT&T Bell Laboratories,
Murray Hill NJ, U.S.A., October 1993.</P>

<p><a href="http://www.research.att.com/sw/tools/graphviz/">Graphviz -
open source graph drawing software</a>. A home page of AT&T's dot and
similar software packages.</p>

</body>
</html> 