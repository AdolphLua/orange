<html><HEAD>
<LINK REL=StyleSheet HREF="style.css" TYPE="text/css" MEDIA=screen>
</HEAD>
<body>
<h1>orngFSS: Orange Feature Subset Selection Module</h1>
<br>

<p>Module orngFSS implements several functions that support or may help
design feature subset selection for classification problems. The guiding idea is that some machine learning methods may perform better if they learn only from a selected subset of "best" features. orngFSS mostly implements filter approaches, i.e., approaches were relevance of attributes is estimate without the knowledge on which
machine learning method will be used to construct a predictive
model.</p>

<hr>

<h2>Implementations</h2>

<h3>Functions</h3>

<dl>
<dt><b>attMeasure</b>(<i>data</i>[<i>, measure</i>])</dt>
<dd class="ddfun">Assesses the quality (relevancy) of attributes using the given measure on a data set <i>data</i> which should contain a discrete class. Returns a sorted list of tuples (attribute name, relevance). <i>measure</i> is an attribute quality measure, which should be derived from <code>orange.MeasureAttribute</code> and defaults to <code>orange.MeasureAttribute_relief(k=20, m=50)</code>.</dd>

<dt><b>bestNAtts</b>(<i>relevancies</i>, <i>N</i>)</dt>
<dd class="ddfun">Returns the list of names of the <i>N</i> highest ranked attributes from the <i>relevancies</i> list. List of attribute measures (<i>relevancies</i>) is of the type as returned by function <code>attMeasure</code>.</dd>

<dt><b>attsAboveThreshold</b>(<i>relevancies</i>[<i>, threshold</i>])</dt>
<dd class="ddfun">Returns the list of names of attributes that are listed in the list <i>relevancies</i> and have their relevance above <i>threshold</i>. The default value for <i>threshold</i> is 0.0.</dd>

<dt><b>selectBestNAtts</b>(<i>data</i>, <i>relevancies</i>, <i>N</i>)</dt>
<dd class="ddfun">Constructs and returns a new set of examples that includes a
class and only <i>N</i> best attributes from a list <i>relevancies</i>. <i>data</i> specifies an original data set.</dd>

<dt><b>selectAttsAboveThresh</b>(<i>data</i>, <i>relevancies</i>[<i>, threshold</i>])</dt>
<dd class="ddfun">Constructs and returns a new set of examples that includes a
  class and attributes from the list returned by function <code>attMeasure</code> that
  have the relevancy above or equal to a specified <i>threshold</i>. <i>data</i> specifies an original data set. Parameter <i>threshold</i> is optional and defaults to 0.0.</dd>


<dt><b>filterRelieff</b>(<i>data</i>[<i>, measure</i>[<em>, margin</em>]])</dt>
<dd class="ddfun">Takes the data set <i>data</i> and a measure for relevancy of attributes <i>measure</i>. Repeats the process of estimating attributes and removing the worst attribute if its measure is lower than <i>margin</i>. Stops when no attribute can be removed. The default for <i>measure</i> is <code>range.MeasureAttribute_relief(k=20, m=50)</code>, and <i>margin</i> defaults to 0.0 Notice that this filter procedure was originally designed for measures such as Relief, which are context dependent, i.e. removal of some attribute usually changes the relevancy of some attribute. Hence the need to re-estimate relevancy every time an attribute is removed. </dd>

</dl>



<h3>Classes</h3>


<dl>
<dt><b>FilterAttsAboveThresh</b>([<em>measure</em>[<em>, threshold</em>]])</dt>
<dd class="ddfun">This is simply a wrapper around the function <code>selectAttsAboveThresh</code>. It allows to create an object which stores filter's parameters and can be later called with the data to return the data set that includes only the selected attributes. <em>measure</em> is a function that returns a list of couples (attribute name, relevance), and it defaults to <code>orange.MeasureAttribute_relief(k=20, m=50)</code>. The default threshold is 0.0. Some examples of how to use this class are:

<xmp>
filter = orngFSS.FilterAttsAboveThresh(threshold=.15)
new_data = filter(data)
new_data = orngFSS.FilterAttsAboveThresh(data)
new_data = orngFSS.FilterAttsAboveThresh(data, threshold=.1)
new_data = orngFSS.FilterAttsAboveThresh(data, threshold=.1,
             measure=orange.MeasureAttribute_gini())
</xmp>
</dd>

<dt><b>FilterBestNAtts</b>([<em>measure</em>[<em>, n</em>]])</dt>
<dd class="ddfun">Similarly to <code>FilterAttsAboveThresh</code>, this is a wrapper around the function <code>selectBestNAtts</code>. Measure and the number of attributes to retain are optional (the latter defaults to 5).</dd>

<dt><b>FilterRelieff</b>([<em>measure</em>[<em>, margin</em>]])</dt>
<dd class="ddfun">Similarly to <code>FilterBestNAtts</code>, this is a wrapper around the function <code>filterRelieff</code>. <em>measure</em> and <em>margin</em> are optional attributes, where <em>measure</em> defaults to <code>orange.MeasureAttribute_relief(k=20, m=50)</code> and <em>margin</em> to 0.0.</dd>

<dt><b>FilteredLearner</b>([<em>basedLearner</em>[<em>, examples</em>[<em>, filter</em>[<em>, name</em>]]]])</dt>
<dd>This class allows to set an learner object, such that before learning feature subset selection using some predefined <em>filter</em> takes place. The learner is build around base learner (any orange learner), which is actually the only attribute we need to specify when we initialize this class. This object comes handy when one wants to test the schema of feature-subset-selection-and-learning by some repeatative evaluation method, e.g., cross validation. Filter defaults to orngFSS.FilterAttsAboveThresh with default attributes. Here is an example of how to set such learner (build a wrapper around naive Bayesian learner) and use it on a data set:</p>
<xmp>
nb = orange.BayesLearner()
learner = orngFSS.FilteredLearner(nb, filter=orngFSS.FilterBestNAtts(n=5), name='filtered')
classifier = learner(data)
</xmp>
</dd>

</dl>

<hr>

<h2>Examples</h2>

<h3>Relevancy Estimation</h3>

Let us start with a simple script that reads the data, uses orngFSS.attMeasure to estimate the relevancy of attributes and print out these for first three attributes. Next, according to this measure, the names of best n=3 attributes are printed.

<p class="header"><a href="fss1.py">fss1.py</a> (uses <a href=
"voting.tab">voting.tab</a>)</p>
<xmp class="code">
import orange, import orngFSS
data = orange.ExampleTable("voting")

print 'Relevance estimate for first three attributes:'
ma = orngFSS.attMeasure(data)
for m in ma[:3]:
  print "%5.3f %s" % (m[1], m[0])

n = 3
best = orngFSS.bestNAtts(ma, n)
print '\nBest %d attributes:' % n
for s in best:
  print s
</xmp>

Depending on a random seed (Relief measure is stochastic!), the above script should output something like:

<xmp class="code">
Relevance estimate for first three attributes:
0.366 physician-fee-freeze
0.164 adoption-of-the-budget-resolution
0.153 crime

Best 3 attributes:
physician-fee-freeze
adoption-of-the-budget-resolution
crime
</xmp>

<h3>Use Different Relevancy Measures</h3>

Just to show how attributes may be measured by some other method, following script uses also gain ratio as an alternative measure. The script outputs both gain ratio and relief on first five attributes. Run the script to see that the ranks of the attributes rather match well!

<p class="header"><a href="fss2.py">fss2.py</a> (uses <a href=
"voting.tab">voting.tab</a>)</p>
<xmp class="code">
import orange, orngFSS
data = orange.ExampleTable("voting")

print 'Relief GainRt Attribute'
ma_def = orngFSS.attMeasure(data)
gainRatio = orange.MeasureAttribute_gainRatio()
ma_gr  = orngFSS.attMeasure(data, gainRatio)
for i in range(5):
  print "%5.3f  %5.3f  %s" % (ma_def[i][1], ma_gr[i][1], ma_def[i][0])
</xmp>

<h3>Filter Approach for Machine Learning</h3>

Feature estimation has at least two potential uses. One is informative (or descriptive): the data analyzer can use measures to see which are her "good" attributes, and which attributes are rather non-informative. For the second one, machine learning techniques can start from the data set that has "worse" attributes removed from original set. This so-called filter approach can boost the performance of learner both in terms of predictive accuracy, speed-up of induction, and simplicity of resulting models.

Following is a script that defines a new classifier, that is based on naive Bayes but selects N (N=5) best attributes from the data set. The new classifier is wrapped-up in a special class (see <a href="../ofb/c_pythonlearner.htm">Building your own learner</a> lesson in <a href="../ofb/default.htm">Orange for Beginners</a>). The script compares this "new" method with a naive Bayes that uses a complete set of attributes.

<p class="header"><a href="fss3.py">fss3.py</a> (uses <a href=
"voting.tab">voting.tab</a>)</p>
<xmp class="code">
import orange, orngFSS
data = orange.ExampleTable("voting")

# first, define a new classifier which will use FSS

def BayesFSS(examples=None, **kwds):
  learner = apply(BayesFSS_Class, (), kwds)
  if examples: return learner(examples)
  else: return learner

class BayesFSS_Class:
  def __init__(self, name='Naive Bayes with FSS', N=5):
    self.name = name
    self.N = N

  def __call__(self, data, weight=None):
    ma = orngFSS.attMeasure(data)
    filtered = orngFSS.selectBestNAtts(data, ma, self.N)
    model = orange.BayesLearner(filtered)
    return BayesFSS_Classifier(classifier = model, nAtts = len(filtered.domain.attributes))

class BayesFSS_Classifier:
  def __init__(self, **kwds):
    self.__dict__ = kwds

  def __call__(self, example, resultType = orange.GetValue):
    return self.classifier(example, resultType)

# test above code on an example
# do 10-fold cross-validation

import orngStat, orngTest
learners = (orange.BayesLearner(name='Naive Bayes'), BayesFSS(name="with FSS"))
results = orngTest.crossValidation(learners, data)

# output the results
print "Learner      CA"
for i in range(len(learners)):
  print "%-12s %5.3f" % (learners[i].name, orngStat.CA(results)[i])
</xmp>

Interestingly, and somehow expected, feature subset selection helps. This is the output that we get:

<xmp class="code">
Learner      CA
Naive Bayes  0.901
with FSS     0.938
</xmp>

<h3>... And a Much Simpler One</h3>

<p>Although perhaps educational, above example was not simple and can be replaced by a simple use of specialized class offered by orngFSS. The main reason for all this complication with classes was that, in order to use to in validation procedures offered by orngEval, we need to wrap a classifier and filter within a single class. Instead of doing this every time (for different base learners and filter methods), orngFSS provides a nice wrapper called <code>FilteredLearner</code> that makes life much simpler. <code>FilteredLearner</code> is an object that is assembled from attribute filter and base learner. When given the data, this learner uses attribute filter to construct a new data set and base learner to construct a classifier. Attribute filters should be classes (like <code>orngFSS.FilterAttsAboveThresh</code> or <code>orngFSS.FilterBestNAtts</code>) that can be initialized with the arguments and later presented with a data, returning new reduced data set.</p>

<p>The following code fragment essentially replaces the bulk of code from previous example, and compares naive Bayesian classifier to the same classifier when only a single most important attribute is used:</p>

<p class="header">from <a href="fss4.py">fss4.py</a> (uses <a href=
"voting.tab">voting.tab</a>)</p>
<xmp class="code">
nb = orange.BayesLearner()
learners = (orange.BayesLearner(name='bayes'),
            FilteredLearner(nb, filter=FilterBestNAtts(n=1), name='filtered'))
results = orngEval.CrossValidation(learners, data)
</xmp>

<p>Now, let's decide to retain three attributes (change the code in <a href="fss4.py">fss4.py</a> accordingly!), but observe how many times an attribute was used. Remember, 10-fold cross validation constructs ten instances for each classifier, and each time we run FilteredLearner a different set of attributes may be selected. <code>orngEval.CrossValidation</code> stores classifiers in <code>results</code> variable, and <code>FilteredLearner</code> returns a classifier that can tell which attributes it used (how convenient!), so the code to do all this is quite short:</p>

<p class="header">from <a href="fss4.py">fss4.py</a> (uses <a href=
"voting.tab">voting.tab</a>)</p>
<xmp class="code">
print "\nNumber of times attributes were used in cross-validation:\n"
attsUsed = {}
for i in range(10):
  for a in results.classifiers[i][1].atts():
    if a.name in attsUsed.keys(): attsUsed[a.name] += 1
    else: attsUsed[a.name] = 1
for k in attsUsed.keys():
  print "%2d x %s" % (attsUsed[k], k)
</xmp>

<p>Running <a href="fss4.py">fss4.py</a> with three attributes selected each time a learner is run gives the following result:</p>

<xmp class="code">
Learner      CA
bayes        0.903
filtered     0.959

Number of times attributes were used in cross-validation:
 3 x el-salvador-aid
 4 x synfuels-corporation-cutback
 8 x adoption-of-the-budget-resolution
10 x physician-fee-freeze
 5 x crime
</xmp>

<p>Experiment yourself to see, if only one attribute is retained for classifier, which attribute was the one most frequently selected over all the ten cross-validation tests!</p>

<hr>

<h2>References</h2>

<p>K. Kira and L. Rendell. A practical approach to feature selection. In D. Sleeman
and P. Edwards, editors, <em>Proc. 9th Int'l Conf. on Machine Learning</em>, pages 249{256,
Aberdeen, 1992. Morgan Kaufmann Publishers.</p>

<p>I. Kononenko. Estimating attributes: Analysis and extensions of RELIEF. In
F. Bergadano and L. De Raedt, editors, <em>Proc. European Conf. on Machine Learning
(ECML-94)</em>, pages 171{182. Springer-Verlag, 1994.</p>

<p>R. Kohavi, G. John: Wrappers for Feature Subset Selection, <em>Artificial Intelligence</em>, 97 (1-2), pages 273-324, 1997</p>
</body>
</html>
